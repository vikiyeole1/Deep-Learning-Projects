{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first module of natural language processing. Natural language processing, also referred to as text analytics, plays a very vital role in today’s era because of the sheer volume of text data that users generate around the world on digital channels such as social media apps, e-commerce websites, blog posts, etc. The first session of this module will take you through the following lectures:\n",
    "\n",
    "- Industry applications of text analytics\n",
    "- Understanding textual data\n",
    "- Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Understanding Text\n",
    "\n",
    "#### Lexical Processing (Converting it to Group of Words)\n",
    "First, you will just convert the raw text into words and, depending on your application's needs, into sentences or paragraphs as well.\n",
    "\n",
    "1. For example, if an email contains words such as lottery, prize and luck, then the email is represented by these words, and it is likely to be a spam email.\n",
    "\n",
    "2. Hence, in general, the group of words contained in a sentence gives us a pretty good idea of what that sentence means. Many more processing steps are usually undertaken in order to make this group more representative of the sentence, for example, cat and cats are considered to be the same word. In general, we can consider all plural words to be equivalent to the singular form.\n",
    "\n",
    "3. For a simple application like spam detection, lexical processing works just fine, but it is usually not enough in more complex applications, like, say, machine translation. For example, the sentences “My cat ate its third meal” and “My third cat ate its meal”, have very different meanings. However, lexical processing will treat the two sentences as equal, as the “group of words” in both sentences is the same. Hence, we clearly need a more advanced system of analysis.\n",
    "\n",
    "#### Syntactic Processing ( Get meanings of the sentence, use grammer)\n",
    "The next step after lexical analysis is where we try to extract more meaning from the sentence, by using its syntax this time. Instead of only looking at the words, we look at the syntactic structures, i.e., the grammar of the language to understand what the meaning is.\n",
    "\n",
    "- One example is differentiating between the subject and the object of the sentence, i.e., identifying who is performing the action and who is the person affected by it. For example, “Ram thanked Shyam” and “Shyam thanked Ram” are sentences with different meanings from each other because in the first instance, the action of ‘thanking’ is done by Ram and affects Shyam, whereas, in the other one, it is done by Shyam and affects Ram. Hence, a syntactic analysis that is based on a sentence’s subjects and objects, will be able to make this distinction.\n",
    "\n",
    "- There are various other ways in which these syntactic analyses can help us enhance our understanding. For example, a question answering system that is asked the question “Who is the Prime Minister of India?”, will perform much better, if it can understand that the words “Prime Minister” are related to “India”. It can then look up in its database, and provide the answer.\n",
    "\n",
    "\n",
    "#### Semantic Processing (Understanding the meaning of relationship between the words)\n",
    "\n",
    "Lexical and syntactic processing don't suffice when it comes to building advanced NLP applications such as language translation, chatbots etc.. The machine, after the two steps given above, will still be incapable of actually understanding the meaning of the text. Such an incapability can be a problem for, say, a question answering system, as it may be unable to understand that PM and Prime Minister mean the same thing. Hence, when somebody asks it the question, “Who is the PM of India?”, it may not even be able to give an answer unless it has a separate database for PMs, as it won’t understand that the words PM and Prime Minister are the same. You could store the answer separately for both the variants of the meaning (PM and Prime Minister), but how many of these meanings are you going to store manually? At some point, your machine should be able to identify synonyms, antonyms, etc. on its own.\n",
    "\n",
    "- This is typically done by inferring the word’s meaning to the collection of words that usually occur around it. So, if the words, PM and Prime Minister occur very frequently around similar words, then you can assume that the meanings of the two words are similar as well.\n",
    "\n",
    "- In fact, this way, the machine should also be able to understand other semantic relations. For example, it should be able to understand that the words “King” and “Queen” are related to each other and that the word “Queen” is simply the female version of the word “King”. Also, both of these words can be clubbed under the word “Monarch”. You can probably save these relations manually, but it will help you a lot more, if you can train your machine to look for the relations on its own, and learn them. Exactly how that training can be done, is something we’ll explore in the third module.\n",
    "\n",
    "Once you have the meaning of the words, obtained via semantic analysis, you can use it for a variety of applications. Machine translation, chatbots and many other applications require a complete understanding of the text, right from the lexical level to the understanding of syntax to that of meaning. Hence, in most of these applications, lexical and semantic processing simply form the “pre-processing” layer of the overall process. In some simpler applications, only lexical processing is also enough as the pre-processing part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers could handle numbers directly and store them on registers (the smallest unit of memory on a computer). But they couldn’t store the non-numeric characters as is. The alphabets and special characters were to be converted to a numeric value first before they could be stored.\n",
    "\n",
    "Hence, the concept of encoding came into existence. All the non-numeric characters were encoded to a number using a code. Also, the encoding techniques had to be standardised so that different computer manufacturers won’t use different encoding techniques.\n",
    "\n",
    "The first encoding standard that came into existence was the ASCII (American Standard Code for Information Interchange) standard, in 1960. ASCII standard assigned a unique code to each character of the keyboard which was known as  ASCII code. For example, the ASCII code of the alphabet ‘A’ is 65 and that of the digit zero is 48. Since then, there have been several revisions made to the codes to incorporate new characters that came into existence after the initial encoding.\n",
    "\n",
    "When ASCII was built, English alphabets were the only alphabets that were present on the keyboard. With time, new languages began to show up on keyboard sets which brought new characters. ASCII became outdated and couldn’t incorporate so many languages. A new standard has come into existence in recent years - the Unicode standard. It supports all the languages in the world - both modern and the older ones.\n",
    "\n",
    "For someone working on text processing, knowing how to handle encodings becomes crucial. Before even beginning with any text processing, you need to know what kind of encoding the text has and if required, modify it to another encoding format.\n",
    "\n",
    "To summarise, there are two most popular encoding standards:\n",
    "- American Standard Code for Information Interchange (ASCII)\n",
    "- Unicode\n",
    "    - UTF-8\n",
    "    - UTF-16\n",
    "    \n",
    "UTF-8 offers a big advantage in cases when the character is an English character or a character from the ASCII character set. Also, while UTF-8 uses only 8 bits to store the character, UTF-16 (BE) uses 16 bits to store it, which looks like a waste of memory.\n",
    "\n",
    "However, in the second case, a symbol is used which doesn’t appear in the ASCII character set. For this case, UTF-8 uses 24 bits, whereas UTF-16 (BE) only uses 16. Hence the storage advantages offered by UTF-8 is reversed and actually becomes a disadvantage here. Also, the advantage UTF-8 offered previously by being same as the ASCII code is also not of use here, as ASCII code doesn’t even exist for this case. The default encoding for strings in python is Unicode UTF-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Regular expressions: Quantifiers - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section onwards, you’ll learn about regular expressions. Regular expressions, also called regex, are very powerful programming tools that are used for a variety of purposes such as feature extraction from text, string replacement and other string manipulations. For someone to become a master at text analytics, being proficient with regular expressions is a must-have skill.\n",
    "\n",
    "A regular expression is a set of characters, or a pattern, which is used to find substrings in a given string. \n",
    "\n",
    "Let’s say you want to extract all the hashtags from a tweet. A hashtag has a fixed pattern to it, i.e. a pound (‘#’) character followed by a string. Some example hashtags are - #mumbai, #bangalore, #upgrad. You could easily achieve this task by providing this pattern and the tweet that you want to extract the pattern from (in this case, the pattern is - any string starting with #). Another example is to extract all the phone numbers from a large piece of textual data.\n",
    "\n",
    "In short, if there’s a pattern in any string, you can easily extract, substitute and do all kinds of other string manipulation operations using regular expressions.\n",
    "\n",
    "Learning regular expressions basically means learning how to identify and define these patterns.\n",
    "\n",
    "Regulars expressions are a language in itself since they have their own compilers. Almost all popular programming languages support working with regexes and so does Python.\n",
    "\n",
    "Regular expression is a set of characters, called as the pattern, which helps in finding substrings in a given string. The pattern is used to detect the substrings\n",
    "\n",
    "For example, suppose you have a dataset of customer reviews about your restaurant. Say, you want to extract the emojis from the reviews because they are a good predictor os the sentiment of the review.\n",
    "\n",
    "Take another example, the artificial assistants such as Siri, Google Now use information retrieval to give you better results. When you ask them for any query or ask them to search for something interesting on the screen, they look for common patterns such as emails, phone numbers, place names, date and time and so on. This is because then the assitant can automatically make a booking or ask you to call the resturant to make a booking.\n",
    "\n",
    "Regular expressions are very powerful tool in text processing. It will help you to clean and handle your text in a much better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import the regular expression library in python.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a quick search using a pattern.\n",
    "re.search('Ravi', 'Ravi is an exceptional student!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ravi\n"
     ]
    }
   ],
   "source": [
    "# print output of re.search()\n",
    "match = re.search('Ravi', 'Ravi is an exceptional student!')\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function to match regular expression patterns\n",
    "def find_pattern(text, patterns):\n",
    "    if re.search(patterns, text):\n",
    "        return re.search(patterns, text)\n",
    "    else:\n",
    "        return 'Not Found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 3), match='abb'>\n"
     ]
    }
   ],
   "source": [
    "### I. Quantifiers\n",
    "# '*': Zero or more \n",
    "print(find_pattern(\"ac\", \"ab*\"))\n",
    "print(find_pattern(\"abc\", \"ab*\"))\n",
    "print(find_pattern(\"abbc\", \"ab*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "# '?': Zero or one (tells whether a pattern is absent or present)\n",
    "print(find_pattern(\"ac\", \"ab?\"))\n",
    "print(find_pattern(\"abc\", \"ab?\"))\n",
    "print(find_pattern(\"abbc\", \"ab?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found!\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 3), match='abb'>\n"
     ]
    }
   ],
   "source": [
    "# '+': One or more\n",
    "print(find_pattern(\"ac\", \"ab+\"))\n",
    "print(find_pattern(\"abc\", \"ab+\"))\n",
    "print(find_pattern(\"abbc\", \"ab+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abb'>\n"
     ]
    }
   ],
   "source": [
    "# {n}: Matches if a character is present exactly n number of times\n",
    "print(find_pattern(\"abbc\", \"ab{2}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 7), match='abbbbb'>\n",
      "Not Found!\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "Not Found!\n"
     ]
    }
   ],
   "source": [
    "# {m,n}: Matches if a character is present from m to n number of times\n",
    "print(find_pattern(\"aabbbbbbc\", \"ab{3,5}\"))   # return true if 'b' is present 3-5 times\n",
    "print(find_pattern(\"aabbbbbbc\", \"ab{7,10}\"))  # return true if 'b' is present 7-10 times\n",
    "print(find_pattern(\"aabbbbbbc\", \"ab{,10}\"))   # return true if 'b' is present atmost 10 times\n",
    "print(find_pattern(\"aabbbbbbc\", \"ab{10,}\"))   # return true if 'b' is present from at least 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='J'>\n",
      "Not Found!\n",
      "<re.Match object; span=(4, 5), match='a'>\n",
      "Not Found!\n"
     ]
    }
   ],
   "source": [
    "### II. Anchors\n",
    "\n",
    "# '^': Indicates start of a string\n",
    "# '$': Indicates end of string\n",
    "\n",
    "print(find_pattern(\"James\", \"^J\"))   # return true if string starts with 'J' \n",
    "print(find_pattern(\"Pramod\", \"^J\"))  # return true if string starts with 'J' \n",
    "print(find_pattern(\"India\", \"a$\"))   # return true if string ends with 'a'\n",
    "print(find_pattern(\"Japan\", \"a$\"))   # return true if string ends with 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='#'>\n"
     ]
    }
   ],
   "source": [
    "### III. Wildcard\n",
    "\n",
    "# '.': Matches any character\n",
    "print(find_pattern(\"a\", \".\"))\n",
    "print(find_pattern(\"#\", \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='c'>\n"
     ]
    }
   ],
   "source": [
    "### IV. Character sets\n",
    "\n",
    "# Now we will look at '[' and ']'.\n",
    "# They're used for specifying a character class, which is a set of characters that you wish to match.\n",
    "# Characters can be listed individually as follows\n",
    "print(find_pattern(\"a\", \"[abc]\"))\n",
    "\n",
    "# Or a range of characters can be indicated by giving two characters and separating them by a '-'.\n",
    "print(find_pattern(\"c\", \"[a-c]\"))  # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found!\n"
     ]
    }
   ],
   "source": [
    "# '^' is used inside character set to indicate complementary set\n",
    "print(find_pattern(\"a\", \"[^abc]\"))  # return true if neither of these is present - a,b or c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character sets\n",
    "| Pattern  | Matches                                                                                    |\n",
    "|----------|--------------------------------------------------------------------------------------------|\n",
    "| [abc]    | Matches either an a, b or c character                                                      |\n",
    "| [abcABC] | Matches either an a, A, b, B, c or C character                                             |\n",
    "| [a-z]    | Matches any characters between a and z, including a and z                                  |\n",
    "| [A-Z]    | Matches any characters between A and Z, including A and Z                                  |\n",
    "| [a-zA-Z] | Matches any characters between a and z, including a and z ignoring cases of the characters |\n",
    "| [0-9]    | Matches any character which is a number between 0 and 9                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta sequences\n",
    "\n",
    "| Pattern  | Equivalent to    |\n",
    "|----------|------------------|\n",
    "| \\s       | [ \\t\\n\\r\\f\\v]    |\n",
    "| \\S       | [^ \\t\\n\\r\\f\\v]   |\n",
    "| \\d       | [0-9]            |\n",
    "| \\D       | [^0-9]           |\n",
    "| \\w       | [a-zA-Z0-9_]     |\n",
    "| \\W       | [^a-zA-Z0-9_]    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 7), match='abbbbb'>\n"
     ]
    }
   ],
   "source": [
    "### Greedy vs non-greedy regex\n",
    "print(find_pattern(\"aabbbbbb\", \"ab{3,5}\")) # return if a is followed by b 3-5 times GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 5), match='abbb'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"aabbbbbb\", \"ab{3,5}?\")) # return if a is followed by b 3-5 times GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 35), match='<HTML><TITLE>My Page</TITLE></HTML>'>\n"
     ]
    }
   ],
   "source": [
    "# Example of HTML code\n",
    "print(re.search(\"<.*>\",\"<HTML><TITLE>My Page</TITLE></HTML>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='<HTML>'>\n"
     ]
    }
   ],
   "source": [
    "# Example of HTML code\n",
    "print(re.search(\"<.*?>\",\"<HTML><TITLE>My Page</TITLE></HTML>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The five most important re functions that you would be required to use most of the times are\n",
    "\n",
    "match() Determine if the RE matches at the beginning of the string\n",
    "\n",
    "search() Scan through a string, looking for any location where this RE matches\n",
    "\n",
    "finall() Find all the substrings where the RE matches, and return them as a list\n",
    "\n",
    "finditer() Find all substrings where RE matches and return them as asn iterator\n",
    "\n",
    "sub() Find all substrings where the RE matches and substitute them with the given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - this function uses the re.match() and let's see how it differs from re.search()\n",
    "def match_pattern(text, patterns):\n",
    "    if re.match(patterns, text):\n",
    "        return re.match(patterns, text)\n",
    "    else:\n",
    "        return ('Not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 3), match='bb'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"abbc\", \"b+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found!\n"
     ]
    }
   ],
   "source": [
    "print(match_pattern(\"abbc\", \"b+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Ramakrishna Rd\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the sub() function. Replace Road with rd.\n",
    "\n",
    "street = '21 Ramakrishna Road'\n",
    "print(re.sub('Road', 'Rd', street))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Rd Rd\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('R\\w+', 'Rd', street))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START - 12END - 20\n",
      "START - 42END - 50\n"
     ]
    }
   ],
   "source": [
    "## Example usage of finditer(). Find all occurrences of word Festival in given sentence\n",
    "\n",
    "text = 'Diwali is a festival of lights, Holi is a festival of colors!'\n",
    "pattern = 'festival'\n",
    "for match in re.finditer(pattern, text):\n",
    "    print('START -', match.start(), end=\"\")\n",
    "    print('END -', match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2017', '10', '28')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of findall(). In the given URL find all dates\n",
    "url = \"http://www.telegraph.co.uk/formula-1/2017/10/28/mexican-grand-prix-2017-time-does-start-tv-channel-odds-lewisl/2017/05/12\"\n",
    "date_regex = '/(\\d{4})/(\\d{1,2})/(\\d{1,2})/'\n",
    "print(re.findall(date_regex, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2017/10/28/\n"
     ]
    }
   ],
   "source": [
    "## Exploring Groups\n",
    "m1 = re.search(date_regex, url)\n",
    "print(m1.group())  ## print the matched group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n"
     ]
    }
   ],
   "source": [
    "print(m1.group(1)) # - Print first group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(m1.group(2)) # - Print second group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(m1.group(3)) # - Print third group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2017/10/28/\n"
     ]
    }
   ],
   "source": [
    "print(m1.group(0)) # - Print zero or the default group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions: Grouping\n",
    "\n",
    "Sometimes you need to extract sub-patterns out of a larger pattern. This can be done by using grouping. Suppose you have textual data with dates in it and you want to extract only the year. from the dates. You can use a regular expression pattern with grouping to match dates and then you can extract the component elements such as the day, month or the year from the date.\n",
    "\n",
    "Grouping is achieved using the parenthesis operators. Let’s understand grouping using an example.\n",
    "\n",
    "Let’s say the source string is: “Kartik’s birthday is on 15/03/1995”. To extract the date from this string you can use the pattern - “\\d{1,2}\\/\\d{1,2}\\/\\d{4}”.\n",
    "\n",
    "Now to extract the year, you can put parentheses around the year part of the pattern. The pattern is: “^\\d{1,2}/\\d{1,2}/(\\d{4})$”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image001.jpg', 'image002.jpg', 'image005.jpg', 'wallpaper.jpg', 'flower.jpg', 'earth.jpg', 'monkey.jpg']\n"
     ]
    }
   ],
   "source": [
    "# items contains all the files and folders of current directory\n",
    "items = ['photos', 'documents', 'videos', 'image001.jpg','image002.jpg','image005.jpg', 'wallpaper.jpg',\n",
    "         'flower.jpg', 'earth.jpg', 'monkey.jpg', 'image002.png']\n",
    "\n",
    "# create an empty list to store resultant files\n",
    "images = []\n",
    "\n",
    "# regex pattern to extract files that end with '.jpg'\n",
    "pattern = \".*\\.jpg$\"\n",
    "\n",
    "for item in items:\n",
    "    if re.search(pattern, item):\n",
    "        images.append(item)\n",
    "\n",
    "# print result\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image001.jpg', 'image002.jpg', 'image005.jpg']\n"
     ]
    }
   ],
   "source": [
    "# items contains all the files and folders of current directory\n",
    "items = ['photos', 'documents', 'videos', 'image001.jpg','image002.jpg','image005.jpg', 'wallpaper.jpg',\n",
    "         'flower.jpg', 'earth.jpg', 'monkey.jpg', 'image002.png']\n",
    "\n",
    "# create an empty list to store resultant files\n",
    "images = []\n",
    "\n",
    "# regex pattern to extract files that start with 'image' and end with '.jpg'\n",
    "pattern = \"image.*\\.jpg$\"\n",
    "\n",
    "for item in items:\n",
    "    if re.search(pattern, item):\n",
    "        images.append(item)\n",
    "\n",
    "# print result\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "Write a regular expression to match all the files that have either .exe, .xml or .jar extensions. A valid file name can contain any alphabet, digit and underscore followed by the extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['employees.xml', 'calculator.jar', 'nfsmw.exe']\n"
     ]
    }
   ],
   "source": [
    "files = ['employees.xml', 'calculator.jar', 'nfsmw.exe', 'bkgrnd001.jpg', 'sales_report.ppt']\n",
    "\n",
    "pattern = \"^.+\\.(xml|jar|exe)$\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for file in files:\n",
    "    match = re.search(pattern, file)\n",
    "    if match !=None:\n",
    "        result.append(file)\n",
    "\n",
    "# print result - result should only contain the items that match the pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Write a regular expression to match all the addresses that have Koramangala embedded in them.\n",
    "\n",
    "Strings that should match:\n",
    "* 466, 5th block, Koramangala, Bangalore\n",
    "* 4th BLOCK, KORAMANGALA - 560034\n",
    "\n",
    "Strings that shouldn't match:\n",
    "* 999, St. Marks Road, Bangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['466, 5th block, Koramangala, Bangalore', '4th BLOCK, KORAMANGALA - 560034']\n"
     ]
    }
   ],
   "source": [
    "addresses = ['466, 5th block, Koramangala, Bangalore', '4th BLOCK, KORAMANGALA - 560034', '999, St. Marks Road, Bangalore']\n",
    "\n",
    "pattern = \"^[\\w\\d\\s,-]*koramangala[\\w\\d\\s,-]*$\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for address in addresses:\n",
    "    match = re.search(pattern, address, re.I)\n",
    "    if match !=None:\n",
    "        result.append(address)\n",
    "\n",
    "# print result - result should only contain the items that match the pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. \n",
    "Write a regular expression that matches either integer numbers or floats upto 2 decimal places.\n",
    "\n",
    "Strings that should match: \n",
    "* 2\n",
    "* 2.3\n",
    "* 4.56\n",
    "* .61\n",
    "\n",
    "Strings that shoudln't match:\n",
    "* 4.567\n",
    "* 75.8792\n",
    "* abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2.3', '4.56', '.61']\n"
     ]
    }
   ],
   "source": [
    "numbers = ['2', '2.3', '4.56', '.61', '4.567', '75.8792', 'abc']\n",
    "\n",
    "pattern = \"^[0-9]*(\\.[0-9]{,2})?$\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for number in numbers:\n",
    "    match = re.search(pattern, number)\n",
    "    if match != None:\n",
    "        result.append(number)\n",
    "\n",
    "# print result - result should only contain the items that match the pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. \n",
    "Write a regular expression to match the model names of smartphones which follow the following pattern: \n",
    "\n",
    "mobile company name followed by underscore followed by model name followed by underscore followed by model number\n",
    "\n",
    "Strings that should match:\n",
    "* apple_iphone_6\n",
    "* samsung_note_4\n",
    "* google_pixel_2\n",
    "\n",
    "Strings that shouldn’t match:\n",
    "* apple_6\n",
    "* iphone_6\n",
    "* google\\_pixel\\_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_iphone_6', 'samsung_note_4', 'google_pixel_2']\n"
     ]
    }
   ],
   "source": [
    "phones = ['apple_iphone_6', 'samsung_note_4', 'google_pixel_2', 'apple_6', 'iphone_6', 'google_pixel_']\n",
    "\n",
    "pattern = \"^.*_.*_\\d$\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for phone in phones:\n",
    "    match = re.search(pattern, phone)\n",
    "    if match !=None:\n",
    "        result.append(phone)\n",
    "\n",
    "# print result - result should only contain the items that match the pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. \n",
    "Write a regular expression that can be used to match the emails present in a database. \n",
    "\n",
    "The pattern of a valid email address is defined as follows:\n",
    "The '@' character can be preceded either by alphanumeric characters, period characters or underscore characters. The length of the part that precedes the '@' character should be between 4 to 20 characters.\n",
    "\n",
    "The '@' character should be followed by a domain name (e.g. gmail.com). The domain name has three parts - a prefix (e.g. 'gmail'), the period character and a suffix (e.g. 'com'). The prefix can have a length between 3 to 15 characters followed by a period character followed by either of these suffixes - 'com', 'in' or 'org'.\n",
    "\n",
    "\n",
    "Emails that should match:\n",
    "* random.guy123@gmail.com\n",
    "* mr_x_in_bombay@gov.in\n",
    "\n",
    "Emails that shouldn’t match:\n",
    "* 1@ued.org\n",
    "* @gmail.com\n",
    "* abc!@yahoo.in\n",
    "* sam_12@gov.us\n",
    "* neeraj@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random.guy123@gmail.com', 'mr_x_in_bombay@gov.in']\n"
     ]
    }
   ],
   "source": [
    "emails = ['random.guy123@gmail.com', 'mr_x_in_bombay@gov.in', '1@ued.org',\n",
    "          '@gmail.com', 'abc!@yahoo.in', 'sam_12@gov.us', 'neeraj@']\n",
    "\n",
    "pattern = \"^[a-z_.0-9]{4,20}@[a-z]{3,15}\\.(com|in|org)$\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for email in emails:\n",
    "    match = re.search(pattern, email, re.I)\n",
    "    if match !=None:\n",
    "        result.append(email)\n",
    "\n",
    "# print result - result should only contain the items that match the pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Lexical Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will learn basic lexical processing. You will get to know the various preprocessing steps you need to apply before you can do any kind of text analytics such as apply machine learning on text, building language models, building chatbots, building sentiment analysis systems and so on. These steps are used in almost all applications that work with textual data. We will also build a spam-ham detector system side-by-side on a very unclean corpus of text. Corpus is just a name to refer to textual data in NLP jargon.\n",
    "\n",
    "Now, you have already built a spam detector while learning about the naive-bayes classifier. Here, you will learn all the preprocessing steps that one needs to do before using a machine learning algorithm on the spam messages dataset. Note that, the preprocessing steps that we teach you here are not limited to building a spam detector.\n",
    "\n",
    "\n",
    "Specifically, you will learn:\n",
    "\n",
    "- How to preprocess text using techniques such as\n",
    "    - Tokenisation\n",
    "    - Stop words removal\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "\n",
    "- How to build a spam detector using one of the following models:\n",
    "    - Bag-of-words model\n",
    "    - TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequencies and Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with any kind of data, the first step that you usually do is to explore and understand it better. In order to explore text data, you need to do some basic preprocessing steps. In the next few segments, you will learn some basic preprocessing and exploratory steps applicable to almost all types of textual data.\n",
    "\n",
    "Now, a text is made of characters, words, sentences and paragraphs. The most basic statistical analysis you can do is to look at the word frequency distribution, i.e. visualising the word frequencies of a given text corpus.\n",
    "\n",
    "It turns out that there is a common pattern you see when you plot word frequencies in a fairly large corpus of text, such as a corpus of news articles, user reviews, Wikipedia articles, etc. In the following lecture, professor Srinath will demonstrate some interesting insights from word frequency distributions. You will also learn what stopwords are and why they are lesser relevant than other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Zipf's law (discovered by the linguist-statistician George Zipf) states that the frequency of a word is inversely proportional to the rank of the word, where rank 1 is given to the most frequent word, 2 to the second most frequent and so on. This is also called the power law distribution.\n",
    "\n",
    "The Zipf's law helps us form the basic intuition for stopwords - these are the words having the highest frequencies (or lowest ranks) in the text, and are typically of limited 'importance'.\n",
    "\n",
    "Broadly, there are three kinds of words present in any text corpus:\n",
    "- Highly frequent words, called stop words, such as ‘is’, ‘an’, ‘the’, etc.\n",
    "- Significant words, which are typically more important to understand the text\n",
    "- Rarely occurring words, which are again less important than significant words\n",
    "\n",
    "\n",
    "Generally speaking, stopwords are removed from the text for two reasons:\n",
    "- They provide no useful information, especially in applications such as spam detector or search engine. Therefore, you’re going to remove stopwords from the spam dataset.\n",
    "- Since the frequency of words is very high, removing stopwords results in a much smaller data as far as the size of data is concerned. Reduced size results in faster computation on text data. There’s also the advantage of less number of features to deal with if stopwords are removed.\n",
    "\n",
    "However, there are exceptions when these words should not be removed. In the next module, you’ll learn concepts such as POS (parts of speech) tagging and parsing where stopwords are preserved because they provide meaningful (grammatical) information in those applications. Generally, stopwords are removed unless they prove to be very helpful in your application or analysis.\n",
    "\n",
    "On the other hand, you’re not going to remove the rarely occurring words because they might provide useful information in spam detection. Also, removing them provides no added efficiency in computation since their frequency is so low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download text of 'Alice in Wonderland' ebook from https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish sharma\\AppData\\Local\\Continuum\\anaconda3\\Python\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "alice = requests.get(url,verify = False).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency(words, top_n=10):\n",
    "    word_freq = FreqDist(words)\n",
    "    labels = [element[0] for element in word_freq.most_common(top_n)]\n",
    "    counts = [element[1] for element in word_freq.most_common(top_n)]\n",
    "    plot = sns.barplot(labels, counts)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot words frequencies present in the gutenberg corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-eba6f7ea9253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malice_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_word_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malice_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "alice_words = alice.text.split()\n",
    "plot_word_frequency(alice_words, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove stopwords from the following piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"the great aim of education is not knowledge but action\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'great', 'aim', 'of', 'education', 'is', 'not', 'knowledge', 'but', 'action']\n"
     ]
    }
   ],
   "source": [
    "sample_words = sample_text.split()\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'aim', 'education', 'knowledge', 'action']\n"
     ]
    }
   ],
   "source": [
    "sample_words = [word for word in sample_words if word not in stopwords.words('english')]\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join words back to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great aim education knowledge action\n"
     ]
    }
   ],
   "source": [
    "sample_text = \" \".join(sample_words)\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords in the genesis corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other things that can be done\n",
    "* Need to change tokens to lower case\n",
    "* Need to get rid of punctuations\n",
    "\n",
    "All the preprocessing steps will be covered while creating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "You already know that you’re going to build a spam detector by the end of this module. In the spam detector application, you’re going to use word tokenisation, i.e. break the text into different words, so that each word can be used as a feature to detect whether the given message is a spam or not.\n",
    "\n",
    "Now, let’s take a look at the spam messages dataset to get a better understanding of how to approach the problem of building a spam detector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
